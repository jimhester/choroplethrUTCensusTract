set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis, predictors)
trainIndex = createDataPartition(diagnosis, p = 3/4)[[1]]
training = adData[trainIndex,]
testing = adData[-trainIndex,]
grep("^IL", names(training), value = TRUE)
preProc <- preProcess(training[grepl("^IL", names(training))], method = "pca", thresh = .9)
preProc
preProc <- preProcess(training[grepl("^IL", names(training))], method = "pca", thresh = .8)
preProc
preProc <- preProcess(training[grepl("^IL", names(training))], method = "pca", thresh = .7)
preProc
preProc <- preProcess(training[grepl("^IL", names(training))], method = "pca", thresh = .9)
preProc
preProc <- preProcess(training[grepl("^IL", names(training))], method = "pca", thresh = .8)
preProc
library(AppliedPredictiveModeling)
library(caret)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis, predictors)
trainIndex = createDataPartition(diagnosis, p = 3/4)[[1]]
training = adData[trainIndex,]
testing = adData[-trainIndex,]
grep("^IL", names(training), value = TRUE)
grepl("^IL", names(training))
grepl("diagnosis|^IL", names(training))
rm(list = ls9)
rm(list = ls())
library(AppliedPredictiveModeling)
library(caret)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis, predictors)
trainIndex = createDataPartition(diagnosis, p = 3/4)[[1]]
training = adData[trainIndex,]
testing = adData[-trainIndex,]
grep("diagnosis|^IL", names(training), value = TRUE)
trainingIL <- training[grepl("diagnosis|^IL", names(training))]
testingIL <- testing[grepl("diagnosis|^IL", names(testing))]
preProc <- preProcess(trainingIL[-1], method = "pca", thresh = .8)
preProc
trainPC <- predict(preProc, trainingIL[-1])
modelFit <- train(trainingIL$diagnosis ~ ., method = "glm", data = trainPC)
testPC <- predict(preProc, testingIL[-1])
confusionMatrix(testing$diagnosis, predict(modelFit, testPC))
install.packages("e1071")
trainPC <- predict(preProc, trainingIL[-1])
modelFit <- train(trainingIL$diagnosis ~ ., method = "glm", data = trainPC)
testPC <- predict(preProc, testingIL[-1])
confusionMatrix(testing$diagnosis, predict(modelFit, testPC))
confusionMatrix(testingIL$diagnosis, predict(modelFit, testPC))
modelFit <- train(diagnosis ~., data = trainingIL, method = "glm")
confusionMatrix(testingIL$diagnosis, predict(modelFit, testingIL))
update.packages()
update.packages()
library(pgmm)
install.packages("pgmm")
library(caret)
library(AppliedPredictiveModeling)
library(ElemStatLearn)
library(pgmm)
library(rpart)
install.packages("ElemStatLearn")
library(caret)
library(AppliedPredictiveModeling)
library(ElemStatLearn)
library(pgmm)
library(rpart)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
head(segmentationOriginal)
names(segmentationOriginal)
segmentationOriginal$Case == "Test"
training = segmentationOriginal[segmentationOriginal$Case == "Train",]
testing = segmentationOriginal[segmentationOriginal$Case == "Test",]
?train
names(getModelInfo())
names(segmentationOriginal)
training = segmentationOriginal[segmentationOriginal$Case == "Train",]
testing = segmentationOriginal[segmentationOriginal$Case == "Test",]
set.seed(125)
modFit <- train(Class ~ ., method = "rpart", data = training)
print(modFit$finalModel)
library(rattle)
fancyRpartPlot(modFit$finalModel)
plot(modFit$finalModel, uniform = TRUE)
text(modFit$finalModel, use.n = TRUE, all = TRUE, cex = .8)
rm(list = ls())
library(pgmm)
data(olive)
olive = olive[,-1]
head(olive)
modFit <- train(Area ~ ., method = "rpart", data = olive)
fancyRpartPlot(modFit$finalModel)
summary(olive$Area)
hist(summary$Area)
hist(olive$Area)
summary(olive$Area)
fancyRpartPlot(modFit$finalModel)
newdata = as.data.frame(t(colMeans(olive)))
predict(modFit, newdata = newdata)
rm(list = ls())
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
names(trainSA)
?SAheart
set.seed(13234)
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl,
method = "lgm", family = "binomial", data = trainSA)
set.seed(13234)
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl,
method = "glm", family = "binomial", data = trainSA)
set.seed(13234)
modFit <- train(I(chd) ~ age + alcohol + obesity + tobacco + typea + ldl,
method = "glm", family = "binomial", data = trainSA)
set.seed(13234)
modFit <- train(as.factor(chd) ~ age + alcohol + obesity + tobacco + typea + ldl,
method = "glm", family = "binomial", data = trainSA)
print(modFit$finalModel)
confusionMatrix(testSA$chd, predict(modFit, testSA))
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(testSA, predict(modFit, testSA))
set.seed(13234)
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl,
method = "glm", family = "binomial", data = trainSA)
print(modFit$finalModel)
confusionMatrix(testSA$chd, predict(modFit, testSA))
rm(list = ls())
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl,
method = "glm", family = "binomial", data = trainSA)
print(modFit$finalModel)
confusionMatrix(testSA$chd, predict(modFit, testSA))
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(testSA, predict(modFit, testSA))
missClass(trainSA, predict(modFit, trainSA))
predict(modFit, newdata = testSA)
head(trainSA)
data(SAheart)
SAheart$chd <- as.factor(SAheart$chd)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
head(trainSA)
set.seed(13234)
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl,
method = "glm", family = "binomial", data = trainSA)
print(modFit$finalModel)
predict(modFit, newdata = testSA)
confusionMatrix(testSA$chd, predict(modFit, testSA))
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(testSA$chd, predict(modFit, testSA))
missClass(as.numeric(testSA$chd), predict(modFit, testSA))
missClass(as.numeric(testSA$chd), as.numeric(predict(modFit, testSA))
)
missClass(as.numeric(testSA$chd), as.numeric(predict(modFit, testSA)))
missClass(as.numeric(trainSA$chd), as.numeric(predict(modFit, trainSA)))
library(ElemStatLearn)
data(SAheart)
#SAheart$chd <- as.factor(SAheart$chd)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl,
method = "glm", family = "binomial", data = trainSA)
print(modFit$finalModel)
confusionMatrix(testSA$chd, predict(modFit, testSA))
sapply(SAheart, class)
predict(modFit, trainSA)
missClass(testSA$chd, predict(modFit, testSA))
missClass(trainSA$chd, predict(modFit, trainSA))
rm(list = ls())
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
head(vowel.test)
head(vowel.train)
vowel.test$y <- as.factor(vowel.test$y)
vowel.train$y <- as.factor(vowel.train$y)
set.seed(33833)
modFit <- train(y ~ ., method = "rf", prox = TRUE, data = vowel.test)
modFit$finalModel
getTree(modFit$finalModel, k = 2)
?varImp
varImp(modFit)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.test$y <- as.factor(vowel.test$y)
vowel.train$y <- as.factor(vowel.train$y)
set.seed(33833)
modFit <- train(y ~ ., method = "rf", prox = TRUE, data = vowel.train)
varImp(modFit)
setwd("~/Google Drive/Data Science/my_twitter_archive")
update.packages()
library(ggplot2)
library(lubridate)
library(scales)
tweets <- read.csv("./july2015archive/tweets.csv", stringsAsFactors = FALSE)
tweets$timestamp <- ymd_hms(tweets$timestamp)
tweets$timestamp <- with_tz(tweets$timestamp, "America/Chicago")
ggplot(data = tweets, aes(x = timestamp)) +
geom_histogram(aes(fill = ..count..)) +
theme(legend.position = "none") +
xlab("Time") + ylab("Number of tweets") +
scale_fill_gradient(low = "midnightblue", high = "aquamarine4")
ggplot(data = tweets, aes(x = wday(timestamp, label = TRUE))) +
geom_histogram(breaks = seq(0.5, 7.5, by =1), aes(fill = ..count..)) +
theme(legend.position = "none") +
xlab("Day of the Week") + ylab("Number of tweets") +
scale_fill_gradient(low = "midnightblue", high = "aquamarine4")
chisq.test(table(wday(tweets$timestamp, label = TRUE)))
table(wday(tweets$timestamp, label = TRUE))
myTable <- table(wday(tweets$timestamp, label = TRUE))
myTable[,2]
myTable[2]
mean(myTable[c(1,6,7)])
mean(myTable[c(2:5)])
mean(myTable[c(2:5)])/mean(myTable[c(1,6,7)])
chisq.test(table(wday(tweets$timestamp, label = TRUE)), p = c(1/7, 1/7, 1/7, 1/7, 1/7, 1/7, 1/7))
chisq.test(table(wday(tweets$timestamp, label = TRUE)), p = c(1, 1, 1, 1, 1, 1, 1, 1)/7)
chisq.test(table(wday(tweets$timestamp, label = TRUE)), p = c(1, 1, 1, 1, 1, 1, 1)/7)
mean(myTable[c(2:5)])/mean(myTable[c(1,6,7)])
mean(myTable[c(2:5)])/mean(myTable[c(1,6,7)])*3
mean(myTable[c(2:5)])/mean(myTable[c(1,6,7)])
mean(myTable[c(2:5)])/mean(myTable[c(1,6,7)])*3
chisq.test(table(wday(tweets$timestamp, label = TRUE)), p = c(3, 4, 4, 4, 4, 3, 3)/25)
chisq.test(table(wday(tweets$timestamp, label = TRUE)), p = c(1, 1, 1, 1, 1, 1, 1)/7)
chisq.test(table(wday(tweets$timestamp, label = TRUE)), p = c(3, 4, 4, 4, 4, 3, 3)/25)
mean(myTable[c(2:5)])/mean(myTable[c(1,6,7)])*3
myTable <- table(wday(tweets$timestamp, label = TRUE))
mean(myTable[c(2:5)])/mean(myTable[c(1,6,7)])
1.25
1.25*4
chisq.test(table(wday(tweets$timestamp, label = TRUE)), p = c(4, 5, 5, 5, 5, 4, 4)/25)
20+4*3
chisq.test(table(wday(tweets$timestamp, label = TRUE)), p = c(4, 5, 5, 5, 5, 4, 4)/32)
mean(myTable[c(2:5)])/mean(myTable[c(1,6,7)])
5/4
tweets <- read.csv("./tweets.csv", stringsAsFactors = FALSE)
tweets$timestamp <- ymd_hms(tweets$timestamp)
tweets$timestamp <- with_tz(tweets$timestamp, "America/Chicago")
ggplot(data = tweets, aes(x = timestamp)) +
geom_histogram(aes(fill = ..count..)) +
theme(legend.position = "none") +
xlab("Time") + ylab("Number of tweets") +
scale_fill_gradient(low = "midnightblue", high = "aquamarine4")
setwd("~/Google Drive/Data Science/blog/juliasilge.github.io/_scripts")
./knitpages.R
source("./knitpages.R")
knit_folder("_R", "_posts", "figs/", "_caches/")
ls
pwd
rm(list = ls())
source("./knitpages.R")
knit_folder("_R", "_posts", "figs/", "_caches/")
source("./knitpages.R")
cd ..
setwd("~/Google Drive/Data Science/blog/juliasilge.github.io")
source("./_scripts/knitpages.R")
source("./_scripts/knitpages.R")
source("./_scripts/knitpages.R")
source("./_scripts/knitpages.R")
source("./_scripts/knitpages.R")
source("./_scripts/knitpages.R")
source("./_scripts/knitpages.R")
tweets <- read.csv("./tweets.csv", stringsAsFactors = FALSE)
source("./_scripts/knitpages.R")
setwd("~/Google Drive/Data Science/blog/juliasilge.github.io")
ls()
source("./_scripts/knitpages.R")
source("./_scripts/knitpages.R")
source("./_scripts/knitpages.R")
tweets <- read.csv("./tweets.csv", stringsAsFactors = FALSE)
getwd()
source("./_scripts/knitpages.R")
source("./_scripts/knitpages.R")
source("./_scripts/knitpages.R")
source("./_scripts/knitpages.R")
source("./_scripts/knitpages.R")
class(tweets)
source("./_scripts/knitpages.R")
source("./_scripts/knitpages.R")
source("./_scripts/knitpages.R")
?render_jekyll
system("which jekyll")
source("./_scripts/knitpages.R")
source("./_scripts/knitpages.R")
source("./_scripts/knitpages.R")
source("./_scripts/knitpages.R")
source("./_scripts/knitpages.R")
source("./_scripts/knitpages.R")
source("./_scripts/knitpages.R")
ggplot(data = tweets, aes(x = year(timestamp))) +
geom_histogram(breaks = seq(2008, 2015, by =1), aes(fill = ..count..)) +
theme(legend.position = "none") +
xlab("Time") + ylab("Number of tweets") +
scale_fill_gradient(low = "midnightblue", high = "aquamarine4")
ggplot(data = tweets, aes(x = year(timestamp))) +
geom_histogram(breaks = seq(2007.5, 2015.5, by =1), aes(fill = ..count..)) +
theme(legend.position = "none") +
xlab("Time") + ylab("Number of tweets") +
scale_fill_gradient(low = "midnightblue", high = "aquamarine4")
source("./_scripts/knitpages.R")
source("./_scripts/knitpages.R")
source("./_scripts/knitpages.R")
pwd
getwd
getwd()
setwd("~/Google Drive/Data Science/blog/juliasilge.github.io")
source("./_scripts/knitpages.R")
source("./_scripts/knitpages.R")
source("./_scripts/knitpages.R")
shiny::runApp('Google Drive/Data Science/datasciencecoursera/DataProducts/dataproductsproject/CountyHealthApp')
shiny::runApp('Google Drive/Data Science/datasciencecoursera/DataProducts/dataproductsproject/CountyHealthApp')
setwd("~/Google Drive/Data Science/datasciencecoursera/DataProducts/dataproductsproject")
runApp(display.mode = "showcase")
setwd("~/Google Drive/Data Science/datasciencecoursera/DataProducts/dataproductsproject/CountyHealthApp")
runApp(display.mode = "showcase")
shiny::runApp()
shiny::runApp()
install.packages("slidify")
library(devtools)
install_github('slidify', 'ramnathv')
install_github('ramnathv/slidifyLibrary')
install_github('ramnathv/slidify')
library(slidify)
install_github('ramnathv/slidifyLibraries')
library(slidifyLibraries)
setwd("~/Google Drive/Data Science/datasciencecoursera/DataProducts/dataproductsproject")
author("UtahCountyHealthDeck")
library(knitr)
slidify('index.Rmd')
browseURL("index.html")
browseURL('index.html')
Test Slide 2
slidify('index.Rmd')
slidify('index.Rmd')
health <- read.csv("./Health_Care_Indicators_By_Counties_In_Utah_2014.csv",
stringsAsFactors = FALSE)
health <- health[c(-1),]
health[,3:67] <- lapply(health[,3:67], as.numeric)
health <- health[,c(2:5,17,18,22,24,27,31,34,38,42,44,48,51,55,60,63,64)]
getwd()
health <- read.csv("./Health_Care_Indicators_By_Counties_In_Utah_2014.csv",
stringsAsFactors = FALSE)
health <- health[c(-1),]
health[,3:67] <- lapply(health[,3:67], as.numeric)
health <- health[,c(2:5,17,18,22,24,27,31,34,38,42,44,48,51,55,60,63,64)]
cor.test(health$Rural, health$Health.Care.Costs)
cor.test(health$Rural, health$Premature.Age.adjusted.Mortality)
For example, for these two indicators (the percentage of county population who are rural and the premature mortality rate), the correlation coefficient is `r round(myCor$estimate, digits = 3)` with a 95% confidence interval from `r round(myCor$conf.int[1], digits = 3` to `r round(myCor$conf.int[2], digits = 3)`.
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
?stat_smooth
names(health)
ggplot(data = health, aes(x = X..18, y = Health.Care.Costs)) +
geom_point(alpha = 0.6, size = 5) +
stat_smooth(method = "lm") +
xlab("Population under 18 (percent)") +
ylab("Median household income (dollars)")
slidify('index.Rmd')
names(health)
ggplot(data = health, aes(x = Median.Household.Income, y = X..Uninsured.1)) +
geom_point(alpha = 0.6, size = 7) +
stat_smooth(method = "lm") +
geom_point(data = subset(health, County == "Salt Lake"), size = 5, colour = "maroon") +
xlab("Median household income (dollars)") +
ylab("Health care costs (dollars per person)")
ggplot(data = health, aes(x = Median.Household.Income,
y = Children.Eligible.Free.Lunch...Free.Lunch)) +
geom_point(alpha = 0.6, size = 5) +
stat_smooth(method = "lm") +
geom_point(data = subset(health, County == "Salt Lake"), size = 7, colour = "maroon") +
xlab("Median household income (dollars)") +
ylab("Health care costs (dollars per person)")
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
slidify('index.Rmd')
publish(title = 'Health Indicators in Utah Counties', 'index.html', host = 'rpubs')
publish(user = "juliasilge", repo = "CountyHealthDeck")
slidify('index.Rmd')
publish(title = 'Health Indicators in Utah Counties', 'index.html', host = 'rpubs')
update.packages()
library(devtools)
library(roxygen2)
setwd("~/Google Drive/Data Science/choroplethrUTCensusTract")
create("choroplethrUTCensusTract")
setwd("./choroplethrUTCensusTract")
document()
document()
library(choroplethr)
document()
library(R6)
?R6Class
document()
pwd
getwd()
setwd(..)
setwd("..")
getwd()
install("choroplethUTCensusTract")
install("choroplethrUTCensusTract")
?ut_tract_choropleth
api.key.install(e4e4a8e70ca6b181686348ae072fde86816661c0)
library(acs)
api.key.install(e4e4a8e70ca6b181686348ae072fde86816661c0)
api.key.install("e4e4a8e70ca6b181686348ae072fde86816661c0")
df_2010_demographics = get_county_demographics(2010)
df_ut_tract_demographics <- get_ut_tract_demographics()
warnings()
hist(df_ut_tract_demographics$total_population)
hist(df_ut_tract_demographics$median_age)
getwd()
devtools::use_data(df_ut_tract_demographics, choroplethrUTCensusTract)
setwd("~/Google Drive/Data Science/choroplethrUTCensusTract/choroplethrUTCensusTract")
devtools::use_data(df_ut_tract_demographics, choroplethrUTCensusTract)
library(choroplethr)
data("df_pop_state")
?df_pop_state
data("df_state_demographics")
?df_state_demographics
install("choroplethrUTCensusTract")
getwd()
setwd("..")
install("choroplethrUTCensusTract")
df_ut_tract_demographics <- get_ut_tract_demographics()
warnings()
devtools::use_data(df_ut_tract_demographics, choroplethrUTCensusTract)
setwd("~/Google Drive/Data Science/choroplethrUTCensusTract/choroplethrUTCensusTract")
devtools::use_data(df_ut_tract_demographics, choroplethrUTCensusTract)
devtools::use_data(df_ut_tract_demographics, choroplethrUTCensusTract, overwrite = TRUE)
df_pop_ut_tract <- df_ut_tract_demographics[,1:2]
colname(df_pop_ut_tract) <- c("region", "value")
colnames(df_pop_ut_tract) <- c("region", "value")
devtools::use_data(df_pop_ut_tract, choroplethrUTCensusTract, overwrite = TRUE)
install.packages("maptools")
library(maptools)
getwd()
setwd("~/Google Drive/Data Science/choroplethrUTCensusTract/choroplethrUTCensusTract/data")
ls
area <- readShapePoly("gz_2010_49_140_00_500k.shp")
library(ggmap)
library(RColorBrewer)
colors <- brewer.pal(9, "BuGn")
mapImage <- get_map(location = c(lon = -111.5, lat = 39.5),
color = "color",
source = "osm",
# maptype = "terrain",
zoom = 6)
library(ggmap)
library(maptools)
mapImage <- get_map(location = c(lon = -111.5, lat = 39.5),
color = "color",
source = "osm",
# maptype = "terrain",
zoom = 6)
detach("package:choroplethr", unload = TRUE)
detach("package:choroplethrUTCensusTract", unload = TRUE)
detach("package:choroplethr", unload = TRUE)
library(ggmap)
